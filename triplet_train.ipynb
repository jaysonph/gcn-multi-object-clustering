{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model.triplet_model import FeatureAggregator\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir: str, frame_ids: list, json_path: str, inp_size:int, orig_im_size=(1080, 1920)) -> None:\n",
    "        self.img_dir = img_dir\n",
    "        self.frame_ids = frame_ids\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.inp_size = inp_size\n",
    "        self.orig_h = orig_im_size[0]\n",
    "        self.orig_w = orig_im_size[1]\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> int:\n",
    "        frame_id = self.frame_ids[idx]\n",
    "        frame_info = self.data[frame_id.replace('_', '/')+'.jpg']\n",
    "        \n",
    "        jk_records = []\n",
    "        sdcl_records = []\n",
    "        cap_records = []\n",
    "        bbox_records = []\n",
    "        jk_imgs = []\n",
    "        sdcl_imgs = []\n",
    "        cap_imgs = []\n",
    "        jk_bboxes = []\n",
    "        sdcl_bboxes = []\n",
    "        cap_bboxes = []\n",
    "        sample_ids = []\n",
    "        for grp_id, detect_results in frame_info.items():\n",
    "            sample_ids.append(idx)\n",
    "            \n",
    "            # Create placeholders for non-existing data\n",
    "            jk_records.append(0)\n",
    "            sdcl_records.append(0)\n",
    "            cap_records.append(0)\n",
    "            jk_imgs.append(torch.zeros((3, self.inp_size, self.inp_size), dtype=torch.float32))\n",
    "            sdcl_imgs.append(torch.zeros((3, self.inp_size, self.inp_size), dtype=torch.float32))\n",
    "            cap_imgs.append(torch.zeros((3, self.inp_size, self.inp_size), dtype=torch.float32))\n",
    "            jk_bboxes.append([-1, -1, -1, -1])\n",
    "            sdcl_bboxes.append([-1, -1, -1, -1])\n",
    "            cap_bboxes.append([-1, -1, -1, -1])\n",
    "            \n",
    "            for obj, bbox in detect_results.items():\n",
    "                bbox = list(map(float, bbox))\n",
    "                bbox[0] = bbox[0]/self.orig_w\n",
    "                bbox[1] = bbox[1]/self.orig_h\n",
    "                bbox[2] = bbox[2]/self.orig_w\n",
    "                bbox[3] = bbox[3]/self.orig_h\n",
    "                img = cv2.imread(os.path.join(self.img_dir, frame_id, obj, f'{grp_id}.jpg'))\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img = Image.fromarray(img)\n",
    "                img = self.preprocess(img)\n",
    "                if obj == 'jockey':\n",
    "                    jk_imgs[-1] = img\n",
    "                    jk_bboxes[-1] = bbox\n",
    "                    jk_records[-1] = 1\n",
    "                if obj == 'sdcl':\n",
    "                    sdcl_imgs[-1] = img\n",
    "                    sdcl_bboxes[-1] = bbox\n",
    "                    sdcl_records[-1] = 1\n",
    "                if obj == 'cap':\n",
    "                    cap_imgs[-1] = img\n",
    "                    cap_bboxes[-1] = bbox\n",
    "                    cap_records[-1] = 1\n",
    "                    \n",
    "        jk_imgs = torch.stack(jk_imgs, dim=0)\n",
    "        sdcl_imgs = torch.stack(sdcl_imgs, dim=0)\n",
    "        cap_imgs = torch.stack(cap_imgs, dim=0)\n",
    "        jk_bboxes = torch.tensor(jk_bboxes)\n",
    "        sdcl_bboxes = torch.tensor(sdcl_bboxes)\n",
    "        cap_bboxes = torch.tensor(cap_bboxes)\n",
    "                \n",
    "        return jk_imgs, sdcl_imgs, cap_imgs, jk_bboxes, sdcl_bboxes, cap_bboxes, jk_records, sdcl_records, cap_records, sample_ids\n",
    "    \n",
    "    def preprocess(self, img: Image.Image) -> torch.tensor:\n",
    "        w, h = img.size\n",
    "        long_edge = max(w, h)\n",
    "        resize_ratio = self.inp_size / long_edge\n",
    "        resize_shape = (round(h*resize_ratio), round(w*resize_ratio))\n",
    "        w_diff, h_diff = (self.inp_size - resize_shape[1]), (self.inp_size - resize_shape[0])\n",
    "        l_pad = w_diff//2\n",
    "        r_pad = w_diff - l_pad\n",
    "        t_pad = h_diff//2\n",
    "        b_pad = h_diff - t_pad\n",
    "        padding = (l_pad, t_pad, r_pad, b_pad)\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(resize_shape),  # interpolation `BILINEAR` is applied by default\n",
    "            transforms.Pad(padding=padding, fill=0, padding_mode='constant'),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        image = transform(img)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    def collate_fn(self, batch_data: list) -> tuple:\n",
    "        jk_imgs, sdcl_imgs, cap_imgs, jk_bboxes, sdcl_bboxes, cap_bboxes, jk_records, sdcl_records, cap_records, sample_ids = zip(*batch_data)\n",
    "        jk_imgs = torch.cat(jk_imgs)\n",
    "        sdcl_imgs = torch.cat(sdcl_imgs)\n",
    "        cap_imgs = torch.cat(cap_imgs)\n",
    "        jk_bboxes = torch.cat(jk_bboxes)\n",
    "        sdcl_bboxes = torch.cat(sdcl_bboxes)\n",
    "        cap_bboxes = torch.cat(cap_bboxes)\n",
    "        jk_records = [i for r in jk_records for i in r]\n",
    "        sdcl_records = [i for r in sdcl_records for i in r]\n",
    "        cap_records = [i for r in cap_records for i in r]\n",
    "        sample_ids = [i for r in sample_ids for i in r]\n",
    "        return jk_imgs, sdcl_imgs, cap_imgs, jk_bboxes, sdcl_bboxes, cap_bboxes, jk_records, sdcl_records, cap_records, sample_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_triplet_dists(\n",
    "    jk_features: torch.tensor, \n",
    "    sdcl_features: torch.tensor, \n",
    "    cap_features: torch.tensor, \n",
    "    jk_records: list, \n",
    "    sdcl_records: list, \n",
    "    cap_records: list, \n",
    "    sample_ids: list, \n",
    "    dist_metric: str\n",
    ") -> tuple(torch.tensor, torch.tensor):\n",
    "    '''\n",
    "    s - len(sample_ids)\n",
    "    \n",
    "    jk_features (torch.float32): [s, f_dim]\n",
    "    sdcl_features (torch.float32): [s, f_dim]\n",
    "    cap_features (torch.float32): [s, f_dim]\n",
    "    jk_records (List): length s\n",
    "    sdcl_records (List): length s\n",
    "    cap_records (List): length s\n",
    "    sample_ids (List): length s\n",
    "    dist_metric: 'cosine' or 'l2'\n",
    "    '''\n",
    "    anc = []\n",
    "    pos = []\n",
    "    neg = []\n",
    "    for i, s_id in enumerate(sample_ids):\n",
    "        jk_exists = jk_records[i]\n",
    "        sdcl_exists = sdcl_records[i]\n",
    "        cap_exists = cap_records[i]\n",
    "        same_s_idcs = np.where(np.array(sample_ids) == s_id)[0]\n",
    "        if len(same_s_idcs) < 2:\n",
    "            continue\n",
    "        \n",
    "        if jk_exists and sdcl_exists:\n",
    "            anc.append(jk_features[i])\n",
    "            pos.append(sdcl_features[i])\n",
    "            neg_id = np.random.choice(np.where(same_s_idcs != i)[0])\n",
    "            obj = np.random.choice(['jk', 'sdcl', 'cap'])\n",
    "            if obj == 'jk':\n",
    "                neg.append(jk_features[neg_id])\n",
    "            elif obj == 'sdcl':\n",
    "                neg.append(sdcl_features[neg_id])\n",
    "            elif obj == 'cap':\n",
    "                neg.append(cap_features[neg_id])\n",
    "                \n",
    "        if sdcl_exists and cap_exists:\n",
    "            anc.append(sdcl_features[i])\n",
    "            pos.append(cap_features[i])\n",
    "            neg_id = np.random.choice(np.where(same_s_idcs != i)[0])\n",
    "            obj = np.random.choice(['jk', 'sdcl', 'cap'])\n",
    "            if obj == 'jk':\n",
    "                neg.append(jk_features[neg_id])\n",
    "            elif obj == 'sdcl':\n",
    "                neg.append(sdcl_features[neg_id])\n",
    "            elif obj == 'cap':\n",
    "                neg.append(cap_features[neg_id])\n",
    "                \n",
    "        if jk_exists and cap_exists:\n",
    "            anc.append(jk_features[i])\n",
    "            pos.append(cap_features[i])\n",
    "            neg_id = np.random.choice(np.where(same_s_idcs != i)[0])\n",
    "            obj = np.random.choice(['jk', 'sdcl', 'cap'])\n",
    "            if obj == 'jk':\n",
    "                neg.append(jk_features[neg_id])\n",
    "            elif obj == 'sdcl':\n",
    "                neg.append(sdcl_features[neg_id])\n",
    "            elif obj == 'cap':\n",
    "                neg.append(cap_features[neg_id])\n",
    "    \n",
    "    # torch.stack to avoid losing grad history\n",
    "    anc = torch.stack(anc)\n",
    "    pos = torch.stack(pos)\n",
    "    neg = torch.stack(neg)\n",
    "    \n",
    "    if dist_metric == 'cosine':\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        pos_dists = 1 - cos(anc, pos)\n",
    "        neg_dists = 1 - cos(anc, neg)\n",
    "    elif dist_metric == 'l2':\n",
    "        l2 = nn.PairwiseDistance(p=2.0, eps=1e-06, keepdim=True)\n",
    "        pos_dists = l2(anc, pos)\n",
    "        neg_dists = l2(anc, neg)\n",
    "    \n",
    "    return pos_dists, neg_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = './data/extracted_data.json'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "inp_size = 64\n",
    "batch_size = 32\n",
    "feat_dim = 256\n",
    "lr = 0.001\n",
    "n_epochs = 1000\n",
    "dist_metric = 'l2'  # 'cosine' or 'l2'\n",
    "margin = 250 if dist_metric == 'l2' else 2\n",
    "\n",
    "model_dst = f'./triplet_models/triplet_{dist_metric}_{feat_dim}'\n",
    "if not os.path.exists(model_dst):\n",
    "    os.makedirs(model_dst)\n",
    "\n",
    "# Train\n",
    "train_img_dir = './data/all_extracted/train'\n",
    "train_img_paths = glob.glob('./data/all_extracted/train/*/*/*.jpg')\n",
    "train_frame_ids = list(set(map(lambda x: x.split('/')[-3], train_img_paths)))\n",
    "train_set = CustomDataset(train_img_dir, train_frame_ids, json_path, inp_size)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=train_set.collate_fn,\n",
    "                          drop_last=True, prefetch_factor=batch_size//2)\n",
    "\n",
    "# val\n",
    "val_img_dir = './data/all_extracted/val'\n",
    "val_img_paths = glob.glob('./data/all_extracted/val/*/*/*.jpg')\n",
    "val_frame_ids = list(set(map(lambda x: x.split('/')[-3], val_img_paths)))\n",
    "val_set = CustomDataset(val_img_dir, val_frame_ids, json_path, inp_size)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=val_set.collate_fn,\n",
    "                        drop_last=True, prefetch_factor=batch_size//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_encoder = models.resnet18(pretrained=True).to(device)\n",
    "img_encoder.train()\n",
    "feat_agg = FeatureAggregator(img_encoder, feat_dim).to(device)\n",
    "\n",
    "loss_fn = nn.MarginRankingLoss(margin=margin)\n",
    "optim = torch.optim.Adam(feat_agg.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, 5, eta_min=0, last_epoch=-1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model):\n",
    "    model.eval()\n",
    "    val_pos_dist = 0\n",
    "    val_neg_dist = 0\n",
    "    val_dist = 0\n",
    "    for bi, data in enumerate(val_loader):\n",
    "        jk_imgs, sdcl_imgs, cap_imgs, jk_bboxes, sdcl_bboxes, cap_bboxes, jk_records, sdcl_records, cap_records, sample_ids = data\n",
    "        jk_imgs, sdcl_imgs, cap_imgs, jk_bboxes, sdcl_bboxes, cap_bboxes = list(map(lambda x: x.to(device), [jk_imgs, sdcl_imgs, cap_imgs, jk_bboxes, sdcl_bboxes, cap_bboxes]))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            jk_feat = model(jk_imgs, jk_bboxes)\n",
    "            sdcl_feat = model(sdcl_imgs, sdcl_bboxes)\n",
    "            cap_feat = model(cap_imgs, cap_bboxes)\n",
    "\n",
    "        pos_dists, neg_dists = gen_triplet_dists(jk_feat, sdcl_feat, cap_feat, jk_records, sdcl_records, cap_records, sample_ids, dist_metric)\n",
    "\n",
    "        val_pos_dist += pos_dists.cpu().mean().item()\n",
    "        val_neg_dist += neg_dists.cpu().mean().item()\n",
    "        val_dist += abs(pos_dists.cpu() - neg_dists.cpu()).mean().item()\n",
    "        \n",
    "    val_pos_dist /= (bi+1)\n",
    "    val_neg_dist /= (bi+1)\n",
    "    val_dist /= (bi+1)\n",
    "    \n",
    "    return val_pos_dist, val_neg_dist, val_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1de8a6f7d347518c2f79026fc6a7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1000:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stats-server/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at epoch 0\n",
      "Tr Loss: 9.29\n",
      "Tr Pos Dist: 11.7811 | Tr Neg Dist: 136.4640 | Tr Dist: 125.1837\n",
      "Val Pos Dist: 34.3587 | Val Neg Dist: 194.5626 | Val Dist: 162.9382 (best: 162.9382)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cab4f8063c2495db89f1e007e5b8148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/1000:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_val_dist = 0\n",
    "for ep in range(n_epochs):\n",
    "    ep_loss = 0\n",
    "    train_pos_dist = 0\n",
    "    train_neg_dist = 0\n",
    "    train_dist = 0\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {ep+1}/{n_epochs}')\n",
    "    feat_agg.train()\n",
    "    for bi, data in enumerate(pbar):\n",
    "        jk_imgs, sdcl_imgs, cap_imgs, jk_bboxes, sdcl_bboxes, cap_bboxes, jk_records, sdcl_records, cap_records, sample_ids = data\n",
    "        jk_imgs, sdcl_imgs, cap_imgs, jk_bboxes, sdcl_bboxes, cap_bboxes = list(map(lambda x: x.to(device), [jk_imgs, sdcl_imgs, cap_imgs, jk_bboxes, sdcl_bboxes, cap_bboxes]))\n",
    "\n",
    "        jk_feat = feat_agg(jk_imgs, jk_bboxes)\n",
    "        sdcl_feat = feat_agg(sdcl_imgs, sdcl_bboxes)\n",
    "        cap_feat = feat_agg(cap_imgs, cap_bboxes)\n",
    "\n",
    "        pos_dists, neg_dists = gen_triplet_dists(jk_feat, sdcl_feat, cap_feat, jk_records, sdcl_records, cap_records, sample_ids, dist_metric)\n",
    "        loss = loss_fn(pos_dists, neg_dists, torch.ones_like(pos_dists)*-1)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        ep_loss += loss.detach().item()\n",
    "        train_pos_dist += pos_dists.cpu().mean().item()\n",
    "        train_neg_dist += neg_dists.cpu().mean().item()\n",
    "        train_dist += abs(pos_dists.cpu() - neg_dists.cpu()).mean()\n",
    "\n",
    "        postfix = {\n",
    "            'Ep Loss': f'{ep_loss/(bi+1):.2f}',\n",
    "            'Tr Pos Dist': f'{train_pos_dist/(bi+1):.4f}',\n",
    "            'Tr Neg Dist': f'{train_neg_dist/(bi+1):.4f}',\n",
    "            'Tr Dist': f'{train_dist/(bi+1):.4f}'\n",
    "        }\n",
    "        pbar.set_postfix(postfix)\n",
    "        \n",
    "    scheduler.step()\n",
    "        \n",
    "    val_pos_dist, val_neg_dist, val_dist = validate(val_loader, feat_agg)\n",
    "    \n",
    "    if val_dist > best_val_dist:\n",
    "        best_val_dist = val_dist\n",
    "        torch.save(feat_agg, os.path.join(model_dst, f'feat_agg_{val_dist:.4f}.pth'))\n",
    "        print(f'Model saved at epoch {ep}')\n",
    "    \n",
    "    print(f'Tr Loss: {ep_loss/(bi+1):.2f}\\nTr Pos Dist: {train_pos_dist/(bi+1):.4f} | Tr Neg Dist: {train_neg_dist/(bi+1):.4f} | \\\n",
    "Tr Dist: {train_dist/(bi+1):.4f}\\nVal Pos Dist: {val_pos_dist:.4f} | Val Neg Dist: {val_neg_dist:.4f} | Val Dist: {val_dist:.4f} (best: {best_val_dist:.4f})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
